<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>Performance Is Not a Number: Avoiding Microbenchmarking Pitfalls</title>

    <link rel="stylesheet" href="reveal.js/css/reveal.css">
    <link rel="stylesheet" href="reveal.js/css/theme/league.css" id="theme">
    <link rel="stylesheet" href="extensions/plugin/line-numbers/line-numbers.css">
    <link rel="stylesheet" href="extensions/css/highlight-styles/zenburn.css">
    <link rel="stylesheet" href="extensions/css/custom.css">

    <style>
      .reveal h1, .reveal h2, .reveal h3, .reveal h4, .reveal h5 { text-transform: none; }
    </style>

    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );

      function set_address(self, remote, local) {
        if (window.location.search.match("local")) {
          self.href = local;
        } else {
          self.href = remote;
        }
      }
    </script>

    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  </head>

  <body>
    <div class="reveal">
      <div class="slides">
          <script type="text/template">
          </script>
          </section>

          <section data-markdown=""
                   data-separator="^====+$"
                   data-separator-vertical="^----+$">
          <script type="text/template">
<!-- .element: data-background-image="images/title_card.png"  data-background-size="100%" -->
<!-- <br />&nbsp; -->
<!-- <br />&nbsp; -->
<!-- <br />&nbsp; -->
<!-- <br />&nbsp; -->
<!-- <br />&nbsp; -->
<!-- <br />&nbsp; -->
<!-- <br />&nbsp; -->

#### Kris Jusiak

### Performance Is Not a Number: Avoiding Microbenchmarking Pitfalls
<img src="images/qr.png" style="width: 15%; background:none; border:none; box-shadow:none;" />

----

#### Disclaimer

#### Focused on x86-64-linux-gnu
<!-- .element: class="fragment" data-fragment-index="2" style="text-align:left" -->
#### Powered by https://github.com/qlibs/perf
<!-- .element: class="fragment" data-fragment-index="2" style="text-align:left" -->

----

#### Performance

```cpp
Speed of light ............................ ~1 foot/ns
L1 cache reference ......................... 0.5 ns
Branch mispredict ............................ 5 ns
L2 cache reference ........................... 7 ns
Mutex lock/unlock ........................... 25 ns
Main memory reference ...................... 100 ns
```

```cpp
Processor         Year      Freq   Transistors
Intel Lunar Lake  2024   4.4 GHz   ?? billions
Apple M4          2024   4.5 GHz   28 billions
AMD Zen 5         2024   5.7 GHz   50 billions
```

##### Modern processors execute nearly as many instructions per cycle as you can supply*
<!-- .element: class="fragment" data-fragment-index="2" style="text-align:left" -->
###### *branching, memory, and input/output
<!-- .element: class="fragment" data-fragment-index="2" style="text-align:left" -->

----

#### Performance Is Not a number!

```cpp
auto fizz_buzz(int n) {
  if (n % 15 == 0) {
    return "FizzBuzz";
  } else if (n % 3 == 0) {
    return "Fizz";
  } else if (n % 5 == 0) {
    return "Buzz";
  } else {
    return "Unknown";
  }
}
```

```
fizz_buzz 123ns // ❓
```
<!-- .element: class="fragment" data-fragment-index="2" style="text-align:left" -->

----

#### Microbenchmarking

##### 'hot spot' engineering can fail, better to reduce overall instruction count
<!-- .element: class="fragment" data-fragment-index="2" style="text-align:left" -->
##### Understanding, test corner cases, tuning
<!-- .element: class="fragment" data-fragment-index="2" style="text-align:left" -->
##### Might be misleading!
<!-- .element: class="fragment" data-fragment-index="2" style="text-align:left" -->

----

### Microbenchmarking - Avoiding Common Pitfalls

----

### [0] Not profiling before

<!-- .slide: data-background="darkblue" -->

----

- use the right tool for the job (don't optimize ns if you are reading from the disk)
  - tactivs vs strategy

- ... (likwid, magictrace, ebpf, xray, coz - https://github.com/plasma-umass/coz)

- can be executed from code

```
[`linux-perf`](https://perf.wiki.kernel.org),
[`intel-vtune`](https://www.intel.com/content/www/us/en/docs/vtune-profiler),
[`amd-uprof`](https://www.amd.com/en/developer/uprof.html),
[`callgrind`](https://valgrind.org/docs/manual/cl-manual.html)
```

  ```cpp
  prof::perf profiler{"/dev/shm/perf"};
  profiler.start();
    // fn
  profiler.stop();
  ```

  - https://github.com/qlibs/prof

----

### [1] Not measuring anything

<!-- .slide: data-background="darkblue" -->

----

- always measure!

Prevent elision

inline constexpr auto prevent_elision = [](auto&& t) -> decltype(auto) {
  if constexpr (std::is_pointer_v<std::remove_cvref_t<decltype(t)>>) {
    asm volatile("" :: "g"(t) : "memory");
  } else {
    #if defined(__clang__)
    asm volatile("" :: "r,m"(t) : "memory");
    #else
    asm volatile("" :: "m,r"(t) : "memory");
    #endif
  }
  return (t);
};

/**
 * returns true if function is elided by the compiler, false otherwise
 */
template<auto Begin = +[]{}, auto End = +[]{}>
[[nodiscard]] inline constexpr auto is_elided(auto&& fn) -> bool {
  const auto invoke = [&] {
    code::label<Begin>();
    fn();
    code::label<End>();
  };
  const auto ptr = &decltype(invoke)::operator();
  prevent_elision(&ptr);
  return code::labels[Begin] == code::labels[End];
}
#endif // PERF_GNU

----

### [2] Assuming measurements follow normal distrubution

<!-- .slide: data-background="darkblue" -->

----

    taking mean!
      - mesuremnets are not following normal distrubution
        - Timing data is usually skewed, not symmetric
      - se min, median, percentials isntead

    show chart of distribution
    show chart of line incorrect

  - statistical appraoch
    median, p90, p95, p99 (latency)

    report

    plot
      - gnuplot charts (sixel) and (console)
        - ecdf! // show on the console output
          ```cpp
          perf::plot::hist
          perf::plot::bar
          perf::plot::box
          perf::plot::line

            important - show error bars (show where the previous one could be slower)

          perf::plot::ecdf // https://en.wikipedia.org/wiki/Empirical_distribution_function

            - running on the server side - sixel

----

### [3] Assuming measurements are independent

<!-- .slide: data-background="darkblue" -->

----

Minimial control
  `pyperf system tune`

Moderate control
  >     # Enable Kernel Mode Task-Isolation (https://lwn.net/Articles/816298)
  >     # cat /sys/devices/system/cpu/isolated
  >     isolcpus=<cpu number>,...,<cpu number>
        - set affinity / set prioroity
        - seperate process

Full control
  - UEFI (wmsr - disable/enable cache (can't be done without user-space))
  - jtags...

----

### [4] Not measureing the right thing

<!-- .slide: data-background="darkblue" -->

----

```cpp
start(); // what is measured?
stop();
```

```cpp
for (
start(); // what is measured?
stop();
```

```cpp
start(); // what is measured?
for (
)
stop();
latency =
```

----

for(..)

----

// This may lead to erroneous conclusions about branch-heavy
// algorithms outperforming branch-free alternatives.
//
// VaryInputs(Measure(Repeat(func)))
// Measure(Repeat(VaryInputs(func)))

latency vs throughput

Latency - time it takes for a single operation to complete (ns)
  cpu -> fpga -> asic

  ```cpp
  template<auto N>
  [[gnu::section("latency")]] [[gnu::align(64u)]] auto latency (auto&& fn, auto&&... ts) {
    auto checksum = 0u;
    perf::code::align(64u); for (auto i = 0u; i < N; ++i) {
      checksum ^= fn(checksum ^ ts...); // data dependency
      perf::memory::synchronize(); // required if there is a memory write
    }
    perf::compiler::prevent_elision(checksum);
  }
  ```

        ```cpp
        latency = ns(time) / operations;
        ```

      >   auto add  = [](int a, int b) { return a + b; };
      >   auto sub  = [](int a, int b) { return a - b; };
      >   auto mult = [](int a, int b) { return a * b; };
      >   auto div  = [](int a, int b) { return a / b; };

    Instruction level
      - nanoBench - https://github.com/andreas-abel/nanoBench
      - nanoBench: A Low-Overhead Tool for Running Microbenchmarks on x86 Systems - https://arxiv.org/abs/1911.03282
      - nanobench (unroll 2x - uroll 1x - link pdf) -> uops.table

        ```cpp
        template<std::size_t N>
        inline constexpr auto unroll = [](auto&& fn) {
          [&]<std::size_t... Ns>(std::index_sequence<Ns...>) {
            (fn(), ...);
          }(std::make_index_sequence<N>{});
        };
        ```

        ```cpp
        time = unroll<N*2>(fn) - unroll<N>(fn);
        ```

        ![uops](images/uops.png)

      - llvm.exegesis (llvm.mca) - https://llvm.org/docs/CommandGuide/llvm-exegesis.html

      - trust your framework (vs mca)

Throughput - total number of operations or tasks completed in a given amount of time (op/s)
  cpu -> gpu/tup

    using perf::bench::policy::seq;
    using perf::bench::policy::unseq;
    using perf::bench::policy::unroll;
    using perf::bench::policy::par;
    using perf::bench::policy::omp;
    using perf::bench::policy::cuda;
      (note) that 10% seq execution can't be parallilzed
        - latency - network request, trade
        - throughput - backtest/llms

        ```
        latency per instruction vs mca
        ```

        ```cpp
        throughput = operations / seconds(time);
        inverse_throughput = ns(time) / operations;
        ```
----

### [5] Not verifying results

<!-- .slide: data-background="darkblue" -->

----

debug verify results / can be done in warmup phase

  ```cpp
  []<auto debug = {}>(std::vector<int>& v) {
    std::ranges::sort(v);
    verify<debug>([] { assert(is_sorted(v); });
  };
  ```

----

### [6] Ignoring hardware effects

<!-- .slide: data-background="darkblue" -->

----

Data distrubtion (branch prediction)

    - input parameters
      ```cpp
      using perf::data::unpredictable;                  // not elided and not predicted
      ```
      - branch prediction

  -fizz_buzz (results with charts)
    - branch prediction, latency vs throughput, alignemnt

    import perf;
    bench...

  - std::sort example
    different sizes
    throughput

    import perf

  Branchy code can do well in synthetic benchmarks, but be careful.

----

### [7] Ignoring compiler effects

<!-- .slide: data-background="darkblue" -->

----

- realsistc scenarios
  alignment, warmup, threading, pin ...

  namespace cpu {
    #if PERF_GNU == 1 and defined(__x86_64__)
    namespace pipeline {
      inline constexpr auto flush = [] {
        asm volatile("cpuid" : : "a"(0), "c"(0) : "ebx", "edx", "memory");
      };
    } // namespace pipeline

  template<class T = std::size_t, class TAllocator = std::allocator<T>>
  inline constexpr auto pollute(const std::size_t size)
    requires (requires { T{}; } or requires { T(size); }) {
    verify(size > 0u);
    std::list<T, TAllocator> data{};
    auto n = size;
    while (n--) {
      if constexpr (requires { T(n); }) {
        data.push_back(T(n));
      } else {
        data.push_back(T{});
      }
    }
  }

  inline constexpr auto pre_fault =
    [](std::span<std::byte> data, const std::size_t page_size = info::sys::page_size()) {
      for (auto i = 0u; i < data.size(); i += page_size) {
        data[i] = {};
      }
    };

  inline constexpr auto flush = [](std::span<const std::byte> data) {
    const auto size = data.size();
    const auto cache_line_size = info::memory::dcache()[info::memory::level::L1].line_size;
    const auto ptr = std::bit_cast<std::uintptr_t>(data.begin());
    const auto aligned_start = ptr & ~(cache_line_size - 1u);
    const auto aligned_end = (ptr + size + cache_line_size - 1u) & ~(cache_line_size - 1u);
    const auto aligned_size = aligned_end - aligned_start;

    #if PERF_GNU == 1 and defined(__x86_64__)
    for (auto i = 0u; i < aligned_size; i += cache_line_size) {
      asm volatile("clflush (%0)" :: "r"(reinterpret_cast<const void*>(aligned_start + i)));
    }
    #elif __has_builtin(__builtin___clear_cache)
    __builtin___clear_cache(
      reinterpret_cast<const std::byte*>(aligned_start),
      reinterpret_cast<const std::byte*>(aligned_start + aligned_size)
    );
    #endif
  };

----

### [8] Not levaraging PMU counters / HW profiling / metrics

<!-- .slide: data-background="darkblue" -->

----

    TSC   - Time Stamp Counter

    Time is discrete: clock cycle
      Processors: 4 GHz (4*10^9 cycles per second)
      One cycle is 0.25 nanoseconds
      light: 7.5 centimeters per cycle
      One byte per cycle: 4 GB/s

    RDPMC - Read Performance Monitoring Counters
    PEBS  - Precise Event-Based Sampling

    - tsc / clock
      tsc = [] {
      };

    - rdpmc (current thread)
      inline constexpr auto rdpmc = [](const std::uint64_t id) {
        std::uint64_t eax{}, edx{};
          asm volatile(
            "rdpmc" : "=a"(eax), "=d"(edx) : "c"(id)
          );
          return ((static_cast<std::uint64_t>(edx)) << 32u) | static_cast<std::uint64_t>(eax);
      };


      ```cpp
      using perf::record::cpu_clock;
      using perf::record::task_clock;
      using perf::record::page_faults;
      using perf::record::faults;
      using perf::record::major_faults;
      using perf::record::minor_faults;
      using perf::record::alignment_faults;
      using perf::record::emulation_faults;
      using perf::record::context_switches;
      using perf::record::cgroup_switches;
      using perf::record::cpu_migrations;
      using perf::record::migrations;
      using perf::record::cycles;
      using perf::record::instructions;
      using perf::record::branch_misses;
      using perf::record::bus_cycles;
      using perf::record::cache_misses;
      using perf::record::cache_references;
      using perf::record::branches;
      using perf::record::branch_instructions;
      using perf::record::stalled_cycles_backend;
      using perf::record::idle_cycles_backend;
      using perf::record::stalled_cycles_frontend;
      using perf::record::idle_cycles_frontend;
      using perf::record::llc_misses;
      using perf::record::l1_misses;
      using perf::record::l1_dcache_loads;
      using perf::record::l1_dcache_load_misses;
      using perf::record::l1_icache_loads;
      using perf::record::l1_icache_load_misses;
      using perf::record::dtlb_loads;
      using perf::record::dtlb_load_misses;
      using perf::record::itlb_loads;
      using perf::record::itlb_load_misses;
      using perf::record::mem_loads;
      using perf::record::mem_stores;
      ```

  perf::profiler profiler{perf::stat::cpu_time, perf::stat:cycles};

  static_assert(profiler::is_syscall_free);

- use commonly recognized metrics
    ```
    /// instruction per cycle (ipc)
    inline constexpr auto ipc = instructions / cycles;

    /// cycles per instruction (cpi, inverse of ipc)
    inline constexpr auto cpi = cycles / instructions;

    /// branch miss rate (branch misses per branch instruction)
    inline constexpr auto branch_miss_rate = branch_misses / branches;

    /// cache miss rate (cache misses per cache reference)
    inline constexpr auto cache_miss_rate = cache_misses / cache_references;

    /// llc miss rate
    inline constexpr auto llc_miss_rate = llc_misses / cache_references;

    /// l1 data cache miss rate
    inline constexpr auto l1_dcache_miss_rate = l1_dcache_load_misses / l1_dcache_loads;

    /// l1 instruction cache miss rate
    inline constexpr auto l1_icache_miss_rate = l1_icache_load_misses / l1_icache_loads;

    /// dtlb miss rate
    inline constexpr auto dtlb_miss_rate = dtlb_load_misses / dtlb_loads;

    /// itlb miss rate
    inline constexpr auto itlb_miss_rate = itlb_load_misses / itlb_loads;

    /// stalled cycles rate (frontend)
    inline constexpr auto frontend_stall_rate = stalled_cycles_frontend / cycles;

    /// stalled cycles rate (backend)
    inline constexpr auto backend_stall_rate = stalled_cycles_backend / cycles;

    /// memory access rate
    inline constexpr auto memory_stall_ratio = stalled_cycles_backend / cycles;

    /// overall stall rate
    inline constexpr auto total_stall_rate = (stalled_cycles_backend + stalled_cycles_frontend) / cycles;

     /// cpu migrations per cycles
    inline constexpr auto cpu_migration_rate = cpu_migrations / cycles;

    /// context switches per cycles
    inline constexpr auto context_switch_rate = context_switches / cycles;

    /// page fault rate
    inline constexpr auto page_fault_rate = faults / cycles;

    /// page fault rate (major faults per total faults)
    inline constexpr auto major_fault_rate = major_faults / cycles;

    /// page fault rate (minor faults per total faults)
    inline constexpr auto minor_fault_rate = minor_faults / cycles;
  } // namespace metric

  - A Top-Down method for performance analysis and counters architecture - https://www.researchgate.net/publication/269302126_A_Top-Down_method_for_performance_analysis_and_counters_architecture

  ![TopDown1](images/tmam1.png)
  ![TopDown2](images/tmam2.png)
    common metric
    memory bound - LLMs also throuhgpu
    cpu bound

    perf[top_down::retirigin]

    ```
    inline constexpr auto retiring = aux::retiring / aux::slots;
    inline constexpr auto heavy_operations = aux::heavy_operations / aux::slots;
    inline constexpr auto light_operations = retiring - heavy_operations;

    inline constexpr auto bad_speculation = aux::bad_speculation / aux::slots;
    inline constexpr auto branch_mispredict = aux::branch_mispredict / aux::slots;
    inline constexpr auto machine_clears = bad_speculation - branch_mispredict;

    inline constexpr auto frontend_bound = aux::frontend_bound / aux::slots;
    inline constexpr auto fetch_latency = aux::fetch_latency / aux::slots;
    inline constexpr auto fetch_bandwidth = frontend_bound - fetch_latency;

    inline constexpr auto backend_bound = aux::backend_bound / aux::slots;
    inline constexpr auto memory_bound = aux::memory_bound / aux::slots;
    inline constexpr auto core_bound = backend_bound - memory_bound;
    ```

----

### [9] Not understanding (profiling/tracing/analyzing) results

<!-- .slide: data-background="darkblue" -->

----

  Want performance - know your hardware!

  IPT   - Intel Processor Trace
  (LBR) - Last Branch Record

  - tracing - intel pt
    ```
    using perf::trace::instructions;
    using perf::trace::cycles;
    ```

  - code labels
    label("foo");
          asm volatile goto(
            ".pushsection labels, \"aw\" \n"
            ".quad %c0, %l[L]\n" /// note: `.quad %c0, 0b` can be reordered
            ".popsection \n"
            : : "i"(Label) : "memory" : L
          ); L:;

    ```cpp
    perf::profiler profiler{perf::trace::instructions, perf::trace::cycles}
    const auto invoke = [&](auto&& fn, auto&&... ts) {
      profiler.start();
      perf::compiler::prevent_elision(fn(ts...));
      profiler.stop();
    };
    invoke(fizz_buzz, std::rand());
    ```

    show ipc per instruction

    - disassemble vs trace vs sample

    ```cpp
        disasm      trace    sample
    1   mov
    2
    3
    4
    5
    6
    ```

  mca
      - llvm-mca - https://llvm.org/docs/CommandGuide/llvm-mca.html - doesn't fetch
        using perf::mca::timeline;                        // produces a detailed report of each instruction’s state transitions through an instruction pipeline
        using perf::mca::resource_pressure;               // reports the average number of resource cycles consumed every iteration by instructions for every processor resource unit available on the target
        using perf::mca::bottleneck;
      - osaca - https://github.com/RRZE-HPC/OSACA
      - uica - https://uica.uops.info

      - stat (perf) vs analyze (mca)

      ```
      ```

      ```cpp
      fizz_buzz(unpredictable)/latency:
            trace.instructions record::mem_loads timeline timeline resource_pressure
      1     1                                      1
      2     2                                      1           1
      3                                                        1
      3
      4     10                                     1
      4     11
      5                                            1
      5     12
      ```

    perf::plot::flamegraph
    ```
    main;init;loadConfig 10
    main;init;checkDependencies 5
    main;start;connectDB 20
    main;start;runQuery 30
    main;start;runQuery;parseResult 25
    main;start;runQuery;logStats 5
    main;shutdown;cleanup 8
    ```

    screeenshots
    ```
    ```cpp
    perf::plot::complexity (big0)
    ```

    - https://flamegraph.com
----

### [10] Not having baseline

<!-- .slide: data-background="darkblue" -->

----

  - baseline

----

### [11] Not tracking results / documenting learnings

<!-- .slide: data-background="darkblue" -->

----

    - learning
    - continous integration
    - testing perf::verify(assembly)
    - jupyter notebook approach

  Eport/Share (jupyter)
    - perf::json
    - code says only what works and not what doesn't
    - what didn't what work
    - share studies!
      - github
    ### [https://github.com/qlibs/perf#studies](https://github.com/qlibs/perf/discussions/4)

----

### Examples

#### [https://github.com/qlibs/perf#studies](https://github.com/qlibs/perf/discussions/4)

----

Techniques
  Data level parallelsim - SWAR, SIMD (AVX512)
  Data oriented design
  Branchless

  ```
  name info
  ---- -------------------------------------------------------
  sys  x86_64-pc-linux-gnu
  cxx  gcc-15.0.0
  cpu  12th Gen Intel(R) Core(TM) i7-12650 (alderlake:6.154.3)
  iL1  32Kb (64b)
  dL1  48Kb/12 (64b)
  frq
  ```

1. Branching
  Branches that jump backward (loops) and are actually taken the first time → taken without miss.
  All other branches (forward jumps that are taken) → mispredict on first encounter.
  - show hot path
  ```
  if (foo) [[likely]] // taken (that's why you can use likely)
  ```
  ```
  [[gnu::always_inline]] inline void fast_path() { std::puts("fast_path"); }
  [[gnu::cold]] void slow_path() { std::puts("slow_path"); }

  constexpr jmp::static_branch<bool> disarmed = false;

  void trigger() {
    if (not disarmed) { // { false: nop, true: jmp }
      fast_path();
    } else {
      slow_path();
    }
  }
  trigger(): // $CXX -O3
    nop                              # code patching (nop->jmp .Ltmp1)
   .Ltmp0:                           # fast path (inlined)
    mov edi, OFFSET FLAT:.LC1
    jmp puts
   .Ltmp1:                           # slow path (cold)
    jmp slow_path() # [clone .cold]
  ```
      // cant measure that with disassembly
  https://github.com/qlibs/jmp

2.  Alignment / latency:
  template<std::align_val_t Alignment>
    requires (std::has_single_bit(std::size_t(Alignment)))
  inline constexpr auto align = [] {
    asm volatile(".align %c0" : : "i"(Alignment));
  };

  code::align<64>

3. dispatching
- mph / magic_lut

  perf::log({
    {"sys",  perf::info::sys::triple()},
    {"cxx",  perf::info::compiler()},
    {"cpu",  perf::info::cpu()},
    {"iL1",  perf::info::memory::icache()},
    {"dL1",  perf::info::memory::dcache()},
    // freq
    // topology
  });


  https://github.com/qlibs/mph

----

##### `"Performance Is Not a Number!"`

##### 0. Not profiling before
<!-- .element: style="text-align:left" -->
##### 1. Not measuring anything
<!-- .element: style="text-align:left" -->
##### 2. Assuming measurements follow normal distrubution
<!-- .element: style="text-align:left" -->
##### 3. Assuming measurements are independent
<!-- .element: style="text-align:left" -->
##### 4. Not measureing the right thing
<!-- .element: style="text-align:left" -->
##### 5. Not verifying results
<!-- .element: style="text-align:left" -->
##### 6. Ignoring hardware effects
<!-- .element: style="text-align:left" -->
##### 7. Ignoring compiler effects
<!-- .element: style="text-align:left" -->
##### 8. Not levaraging PMU counters / HW profiling / metrics
<!-- .element: style="text-align:left" -->
##### 9. Not understanding (profiling/tracing/analyzing) results
<!-- .element: style="text-align:left" -->
##### 10. Not having baseline
<!-- .element: style="text-align:left" -->
##### 11. Not tracking results / documenting learnings
<!-- .element: style="text-align:left" -->

----

### Further readings

<img src="images/qr.png" style="width: 15%; background:none; border:none; box-shadow:none;" />

#### [https://github.com/qlibs/perf#resources](https://github.com/qlibs/perf?tab=readme-ov-file#User-Guide)

          </script>
        </section>

      </div>
    </div>

    <script src="reveal.js/lib/js/head.min.js"></script>
    <script src="reveal.js/js/reveal.js"></script>

    <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({

        // Display controls in the bottom right corner
        controls: false,

        // Display a presentation progress bar
        progress: false,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // Hides the address bar on mobile devices
        hideAddressBar: true,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style
        transition: 'none', // none/fade/slide/convex/concave/zoom

        // Transition speed
        transitionSpeed: 'default', // default/fast/slow

        // Transition style for full page slide backgrounds
        backgroundTransition: 'none', // none/fade/slide/convex/concave/zoom

        // Number of slides away from the current that are visible
        viewDistance: 1,

        // Parallax background image
        parallaxBackgroundImage: '', // e.g. "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'"

        // Parallax background size
        parallaxBackgroundSize: '', // CSS syntax, e.g. "2100px 900px"

        // Number of pixels to move the parallax background per slide
        // - Calculated automatically unless specified
        // - Set to 0 to disable movement along an axis
        parallaxBackgroundHorizontal: null,
        parallaxBackgroundVertical: null,

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true },
          { src: 'extensions/plugin/line-numbers/line-numbers.js' }
        ]
      });

      function handleClick(e) {
        if (1 >= outerHeight - innerHeight) {
          document.querySelector( '.reveal' ).style.cursor = 'none';
        } else {
          document.querySelector( '.reveal' ).style.cursor = '';
        }

        e.preventDefault();
        if(e.button === 0) Reveal.next();
        if(e.button === 2) Reveal.prev();
      }
    </script>

  </body>
</html>
