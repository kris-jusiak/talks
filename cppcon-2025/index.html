<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>Performance Is Not a Number: Avoiding Microbenchmarking Pitfalls</title>

    <link rel="stylesheet" href="reveal.js/css/reveal.css">
    <link rel="stylesheet" href="reveal.js/css/theme/league.css" id="theme">
    <link rel="stylesheet" href="extensions/plugin/line-numbers/line-numbers.css">
    <link rel="stylesheet" href="extensions/css/highlight-styles/zenburn.css">
    <link rel="stylesheet" href="extensions/css/custom.css">

    <style>
      .reveal h1, .reveal h2, .reveal h3, .reveal h4, .reveal h5 { text-transform: none; }
    </style>

    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );

      function set_address(self, remote, local) {
        if (window.location.search.match("local")) {
          self.href = local;
        } else {
          self.href = remote;
        }
      }
    </script>

    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  </head>

  <body>
    <div class="reveal">
      <div class="slides">
          <script type="text/template">
          </script>
          </section>

          <section data-markdown=""
                   data-separator="^====+$"
                   data-separator-vertical="^----+$">
          <script type="text/template">
<!-- .element: data-background-image="images/title_card.png"  data-background-size="100%"  -->

<br />&nbsp;
<br />&nbsp;
<br />&nbsp;
<br />&nbsp;
<br />&nbsp;
<br />&nbsp;
<br />&nbsp;
<img src="images/qr.png" style="width: 20%; background:none; border:none; box-shadow:none;" />

----

Disclaimer

  ```
  name info                                                    
  ---- ------------------------------------------------------- 
  sys  x86_64-pc-linux-gnu 
  cxx  gcc-15.0.0 
  cpu  12th Gen Intel(R) Core(TM) i7-12650 (alderlake:6.154.3) 
  iL1  32Kb (64b) 
  dL1  48Kb/12 (64b) 
  frq
  ```

 Powered by
  - https://github.com/qlibs/perf

 Legend
  - BLUE - Learning

---

### Motivation

Why microbenchmarking

- hard to understand corner cases

fizz_buzz 32ns

----

Learnings
- always measure
- use the right tool for the job (don't optimize ns if you are reading from the disk)
  - tactivs vs strategy
- performance is not a number
- microbenchmarking
  - baseline
  - trust your framework (vs mca)
- realsistc scenarios (alignment, warmup, threading, pin ...)
- latency vs throughput (policy)
- vary input data distrubtion
- statistic measurements (median, mean)
- charts (ecdf, etc...)
- understand what results means - profiling / know your hardware
    Computer Architecture - Onur Mutlu - https://www.youtube.com/@OnurMutluLectures
    Computer, Enhance - Casey Muratori
    Ignas Bagdonas - talks
- use commonly recognized metrics
  - top-down (show picture)
    memory bound - LLMs alsso throuhgpu
    cpu bound
  - ipc, ...
- document leraning (what didn't what work)

----

Terminology
  PEBS
  IPT / intel has the most capabilities
  - ipt intel / Apple M4
 (clang/gcc, llvm, linux/perf, libipt, gnuplot, ...) 
  ...

Agenda

-why (the quantum mechanics of micro-benchmarking)

  number of tranisiotn - from lemior slieds


  ```cpp
    Speed of light ............................ ~1 foot/ns
    L1 cache reference ......................... 0.5 ns
    Branch mispredict ............................ 5 ns
    L2 cache reference ........................... 7 ns
    Mutex lock/unlock ........................... 25 ns
    Main memory reference ...................... 100 ns
  ```
  -fizz_buzz (results with charts)
    - branch prediction, latency vs throughput, alignemnt

      latency: cpu -> fpga -> asic
      thropugput : cpu -> gpu/tup

    import perf;
    bench...

  - std::sort example
    different sizes
    throughput

    import perf

-how

  perf::log({
    {"sys",  perf::info::sys::triple()},
    {"cxx",  perf::info::compiler()},
    {"cpu",  perf::info::cpu()},
    {"iL1",  perf::info::memory::icache()},
    {"dL1",  perf::info::memory::dcache()},
    // freq
    // topology
  });

  - bench
        Highel level
        - design/algos (BigO, coz, etc...)

        Function Level
          - google-benchmark - https://github.com/google/benchmark / https://quick-bench.com
          - nanobench - https://github.com/martinus/nanobench
          - celero - https://github.com/DigitalInBlue/Celero

        Instruction level
        - nanobench (unroll 2x - uroll 1x - link pdf) -> uops.table
            template<std::size_t N>
            inline constexpr auto unroll = [](auto&& fn) {
              const auto invoke = [&]<std::size_t I> {
                if constexpr (requires { fn.template operator()<std::size_t{}>(); }) {
                  fn.template operator()<I>();
                } else {
                  fn();
                }
              };
              [&]<std::size_t... Ns>(std::index_sequence<Ns...>) {
                (invoke.template operator()<Ns>(), ...);
              }(std::make_index_sequence<N>{});
            };

            - nanoBench - https://github.com/andreas-abel/nanoBench
            - nanoBench: A Low-Overhead Tool for Running Microbenchmarks on x86 Systems - https://arxiv.org/abs/1911.03282

            latency vs throughput for instruction

            todo screenshot

        - llvm.exegesis (llvm.mca)
          - https://llvm.org/docs/CommandGuide/llvm-exegesis.html
        - uarch-bench - https://github.com/travisdowns/uarch-bench

      - latency vs throughput
        plus different process (for isolatotion)


        ```
        latency per instruction vs mca
        ```

      >   auto add  = [](int a, int b) { return a + b; };
      >   auto sub  = [](int a, int b) { return a - b; };
      >   auto mult = [](int a, int b) { return a * b; };
      >   auto div  = [](int a, int b) { return a / b; };


       - throughput
          using perf::bench::policy::seq;
          using perf::bench::policy::unseq;
          using perf::bench::policy::unroll;
          using perf::bench::policy::par;
          using perf::bench::policy::omp;
          using perf::bench::policy::cuda;
      (note) that 10% seq execution can't be parallilzed
        - latency - network request, trade
        - throughput - backtest/llms
    - input parameters
      ```cpp
      using perf::data::unpredictable;                  // not elided and not predicted
      ```
      - branch prediction
    - runs statistical appraoch
      - median, p90, p95, p99 (latency)
      - gnuplot charts (sixel) and (console)
        - ecdf! // show on the console output
          ```cpp
          perf::plot::hist
          perf::plot::bar
          perf::plot::box
          perf::plot::line
          perf::plot::ecdf // https://en.wikipedia.org/wiki/Empirical_distribution_function
          perf::plot::flamegraph
          screeenshots
          ```
          ```cpp
          perf::plot::complexity (big0)
          ```

          servser side (no logs)

        - jupyter notebook approach

    - baseline
    - ipt (intel, m4) -> event_open -> mca -> flamegraphs
      - llvm-mca - https://llvm.org/docs/CommandGuide/llvm-mca.html - doesn't fetch
        using perf::mca::timeline;                        // produces a detailed report of each instructionâ€™s state transitions through an instruction pipeline
        using perf::mca::resource_pressure;               // reports the average number of resource cycles consumed every iteration by instructions for every processor resource unit available on the target
        using perf::mca::bottleneck;          
      - osaca - https://github.com/RRZE-HPC/OSACA
      - uica - https://uica.uops.info

    -isolation / noise reduction
        - set affinity / set prioroity
        - seperate process
        - disable turbo hyperthreading
        - isoplus 
        - `pyperf system tune`
      - PERF_UEFI
        - no os
        - disable/enable cache (can't be done without user-space)

      - jtags...
    - profilers
      - time
        - tsc / clock
        - rdpmc (current thread)

      perf::profiler profiler{perf::stat::cpu_time, perf::stat:cycles};

      static_assert(profiler::is_syscall_free);

      start()
      ```
      rdtsc
      rdpmc
      ```

        ```cpp
        latency = ns(time) / operations;
        ```

        ```cpp
        throughput = operations / seconds(time);
        inverse_throughput = ns(time) / operations;
        ```

      - code labels
        label("foo");
              asm volatile goto(
                ".pushsection labels, \"aw\" \n"
                ".quad %c0, %l[L]\n" /// note: `.quad %c0, 0b` can be reordered
                ".popsection \n"
                : : "i"(Label) : "memory" : L
              ); L:;

        - prevent_elision/is_elided

        inline constexpr auto prevent_elision = [](auto&& t) -> decltype(auto) {
          if constexpr (std::is_pointer_v<std::remove_cvref_t<decltype(t)>>) {
            asm volatile("" :: "g"(t) : "memory");
          } else {
            #if defined(__clang__)
            asm volatile("" :: "r,m"(t) : "memory");
            #else
            asm volatile("" :: "m,r"(t) : "memory");
            #endif
          }
          return (t);
        };

        /**
         * returns true if function is elided by the compiler, false otherwise
         */
        template<auto Begin = +[]{}, auto End = +[]{}>
        [[nodiscard]] inline constexpr auto is_elided(auto&& fn) -> bool {
          const auto invoke = [&] {
            code::label<Begin>();
            fn();
            code::label<End>();
          };
          const auto ptr = &decltype(invoke)::operator();
          prevent_elision(&ptr);
          return code::labels[Begin] == code::labels[End];
        }
        #endif // PERF_GNU


        - how to get functoin address

      - code markers

      - stat/record
        ```cpp
        using perf::record::cpu_clock;
        using perf::record::task_clock;
        using perf::record::page_faults;
        using perf::record::faults;
        using perf::record::major_faults;
        using perf::record::minor_faults;
        using perf::record::alignment_faults;
        using perf::record::emulation_faults;
        using perf::record::context_switches;
        using perf::record::cgroup_switches;
        using perf::record::cpu_migrations;
        using perf::record::migrations;
        using perf::record::cycles;
        using perf::record::instructions;
        using perf::record::branch_misses;
        using perf::record::bus_cycles;
        using perf::record::cache_misses;
        using perf::record::cache_references;
        using perf::record::branches;
        using perf::record::branch_instructions;
        using perf::record::stalled_cycles_backend;
        using perf::record::idle_cycles_backend;
        using perf::record::stalled_cycles_frontend;
        using perf::record::idle_cycles_frontend;
        using perf::record::llc_misses;
        using perf::record::l1_misses;
        using perf::record::l1_dcache_loads;
        using perf::record::l1_dcache_load_misses;
        using perf::record::l1_icache_loads;
        using perf::record::l1_icache_load_misses;
        using perf::record::dtlb_loads;
        using perf::record::dtlb_load_misses;
        using perf::record::itlb_loads;
        using perf::record::itlb_load_misses;
        using perf::record::mem_loads;
        using perf::record::mem_stores;
        ```

        ```
        /// instruction per cycle (ipc)
        inline constexpr auto ipc = instructions / cycles;

        /// cycles per instruction (cpi, inverse of ipc)
        inline constexpr auto cpi = cycles / instructions;

        /// branch miss rate (branch misses per branch instruction)
        inline constexpr auto branch_miss_rate = branch_misses / branches;

        /// cache miss rate (cache misses per cache reference)
        inline constexpr auto cache_miss_rate = cache_misses / cache_references;

        /// llc miss rate
        inline constexpr auto llc_miss_rate = llc_misses / cache_references;

        /// l1 data cache miss rate
        inline constexpr auto l1_dcache_miss_rate = l1_dcache_load_misses / l1_dcache_loads;

        /// l1 instruction cache miss rate
        inline constexpr auto l1_icache_miss_rate = l1_icache_load_misses / l1_icache_loads;

        /// dtlb miss rate
        inline constexpr auto dtlb_miss_rate = dtlb_load_misses / dtlb_loads;

        /// itlb miss rate
        inline constexpr auto itlb_miss_rate = itlb_load_misses / itlb_loads;

        /// stalled cycles rate (frontend)
        inline constexpr auto frontend_stall_rate = stalled_cycles_frontend / cycles;

        /// stalled cycles rate (backend)
        inline constexpr auto backend_stall_rate = stalled_cycles_backend / cycles;

        /// memory access rate
        inline constexpr auto memory_stall_ratio = stalled_cycles_backend / cycles;

        /// overall stall rate
        inline constexpr auto total_stall_rate = (stalled_cycles_backend + stalled_cycles_frontend) / cycles;

         /// cpu migrations per cycles
        inline constexpr auto cpu_migration_rate = cpu_migrations / cycles;

        /// context switches per cycles
        inline constexpr auto context_switch_rate = context_switches / cycles;

        /// page fault rate
        inline constexpr auto page_fault_rate = faults / cycles;

        /// page fault rate (major faults per total faults)
        inline constexpr auto major_fault_rate = major_faults / cycles;

        /// page fault rate (minor faults per total faults)
        inline constexpr auto minor_fault_rate = minor_faults / cycles;
      } // namespace metric
        ```

      - top_down
        ```
        inline constexpr auto retiring = aux::retiring / aux::slots;
        inline constexpr auto heavy_operations = aux::heavy_operations / aux::slots;
        inline constexpr auto light_operations = retiring - heavy_operations;

        inline constexpr auto bad_speculation = aux::bad_speculation / aux::slots;
        inline constexpr auto branch_mispredict = aux::branch_mispredict / aux::slots;
        inline constexpr auto machine_clears = bad_speculation - branch_mispredict;

        inline constexpr auto frontend_bound = aux::frontend_bound / aux::slots;
        inline constexpr auto fetch_latency = aux::fetch_latency / aux::slots;
        inline constexpr auto fetch_bandwidth = frontend_bound - fetch_latency;

        inline constexpr auto backend_bound = aux::backend_bound / aux::slots;
        inline constexpr auto memory_bound = aux::memory_bound / aux::slots;
        inline constexpr auto core_bound = backend_bound - memory_bound;
        ```

        - A Top-Down method for performance analysis and counters architecture - https://www.researchgate.net/publication/269302126_A_Top-Down_method_for_performance_analysis_and_counters_architecture
      - tracing - intel pt
        ```
        using perf::trace::instructions;
        using perf::trace::cycles;
        ```

        ```cpp
        perf::profiler profiler{perf::trace::instructions, perf::trace::cycles}
        const auto invoke = [&](auto&& fn, auto&&... ts) {
          profiler.start();
          perf::compiler::prevent_elision(fn(ts...));
          profiler.stop();
        };
        invoke(fizz_buzz, std::rand());
        ```

        - stat (perf) vs analyze (mca)

        ```
        ```

        - disassemble vs trace vs sample

        ```cpp
            disasm      trace    sample 
        1   mov 
        2
        3
        4
        5
        6
        ```

      - simualtion - callgrind
      - profiling - perf (vtune, uprof...)
          /**
           * perf record --control=fifo:/dev/shm/perf --delay=-1 ./a.out
           */
          class perf {
            static constexpr auto enable = "enable\n";
            static constexpr auto disable = "disable\n";

           public:
            constexpr explicit perf(std::string&& control)
              : fd_{open(control.c_str(), O_WRONLY)}
            { }
            constexpr perf(const perf&) = delete;
            constexpr perf(perf&& other)
              : fd_{std::move(other.fd_)} {
              other.fd_ = -1;
            }
            constexpr ~perf() noexcept {
              if (fd_ == -1) return;
              close(fd_);
            }

            constexpr void start() noexcept {
              write(fd_, enable, sizeof(enable));
            }

            constexpr void stop() noexcept {
              write(fd_, disable, sizeof(disable));
            }

           private:
            int fd_{};
          };

          ```cpp
          [`linux-perf`](https://perf.wiki.kernel.org),
          [`intel-vtune`](https://www.intel.com/content/www/us/en/docs/vtune-profiler),
          [`amd-uprof`](https://www.amd.com/en/developer/uprof.html),
          [`callgrind`](https://valgrind.org/docs/manual/cl-manual.html)
          ```

          ```cpp
          start();
          stop();
          ```

          https://github.com/qlibs/prof

          likwid
          magictrace
          ebpf

        Production
          not the code
            - linux-perf - https://www.brendangregg.com/perf.html
            - perf probe (intel-pt)
              - https://man7.org/linux/man-pages/man1/perf-probe.1.html
            - Recording Inferiorâ€™s Execution and Replaying It
              - https://sourceware.org/gdb/current/onlinedocs/gdb.html/Process-Record-and-Replay.html

          changing the code
            - [`llvm-xray`](https://llvm.org/docs/XRay.html),
              - rdpmc

    inline constexpr auto rdpmc = [](const std::uint64_t id) {
      std::uint64_t eax{}, edx{};
        asm volatile(
          "rdpmc" : "=a"(eax), "=d"(edx) : "c"(id)
        );
        return ((static_cast<std::uint64_t>(edx)) << 32u) | static_cast<std::uint64_t>(eax);
    };

              ```cpp
              [[clang::xray_always_instrument]]
              auto always() { return 42; }

              [[clang::xray_always_instrument, clang::xray_log_args(1)]]
              auto always_profile_and_log_i(int i) { return i; }

              [[clang::xray_never_instrument]]
              auto never() { return 42; }

              always():
                      nop     word ptr [rax + rax + 512]
                      mov     eax, 42
                      ret
                      nop     word ptr cs:[rax + rax + 512]

              always_profile_and_log_i(int):
                      nop     word ptr [rax + rax + 512]
                      mov     eax, edi
                      ret
                      nop     word ptr cs:[rax + rax + 512]

              never():
                      mov     eax, 42
                      ret
              ```

            - custom markers (aka xray)
              marker("foo")
                -> nop

            - linux-perf

        Notes
          - debug verify results / can be done in warmup phase

            ```cpp
            []<auto debug = {}>(std::vector<int>& v) {
              std::ranges::sort(v);
              verify<debug>([] { assert(is_sorted(v); });
            };
            ```

        flamegraphs
        format is super easy
        flamegraph.txt
        ```
        main;init;loadConfig 10
        main;init;checkDependencies 5
        main;start;connectDB 20
        main;start;runQuery 30
        main;start;runQuery;parseResult 25
        main;start;runQuery;logStats 5
        main;shutdown;cleanup 8
        ```

        - https://flamegraph.com

        Eport/Share (jupyter)
          - code says only what works and not what doesn't
          - share studies!

        ### [https://github.com/qlibs/perf#studies](https://github.com/qlibs/perf/discussions/4)

- what (examples)

  Example (dispatch techniques)
    - full analyze

  ```cpp
  fizz_buzz(unpredictable)/latency:
        trace.instructions record::mem_loads timeline timeline resource_pressure
  1     1                                      1
  2     2                                      1           1
  3                                                        1
  3
  4     10                                     1
  4     11
  5                                            1
  5     12
  ```


  - alignment (latency)
  - memory (ipc)
  - mph / magic_lut
  - [[gnu::target("avx2")]]
  - [[gnu::target("bmi2")]]
  - C++26 - show how to pack structs r soa

----

### Summary

> ##### `"Performance Is Not a Number!"`

- always measure
- use the right tool for the job (don't optimize ns if you are reading from the disk)
- performance is not a number
- realsistc scenarios (alignment, warmup, threading, pin ...)
- latency vs throughput (policy)
- vary input data distrubtion
- statistic measurements (median, mean)
- charts (ecdf, etc...)
- understand what results means - profiling / know your hardware
- use commonly recognized metrics
- document leraning (what didn't what work)

----

### Further readings

### [https://github.com/qlibs/perf#resources](https://github.com/qlibs/perf?tab=readme-ov-file#User-Guide)

          </script>
        </section>

      </div>
    </div>

    <script src="reveal.js/lib/js/head.min.js"></script>
    <script src="reveal.js/js/reveal.js"></script>

    <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({

        // Display controls in the bottom right corner
        controls: false,

        // Display a presentation progress bar
        progress: false,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // Hides the address bar on mobile devices
        hideAddressBar: true,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style
        transition: 'none', // none/fade/slide/convex/concave/zoom

        // Transition speed
        transitionSpeed: 'default', // default/fast/slow

        // Transition style for full page slide backgrounds
        backgroundTransition: 'none', // none/fade/slide/convex/concave/zoom

        // Number of slides away from the current that are visible
        viewDistance: 1,

        // Parallax background image
        parallaxBackgroundImage: '', // e.g. "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'"

        // Parallax background size
        parallaxBackgroundSize: '', // CSS syntax, e.g. "2100px 900px"

        // Number of pixels to move the parallax background per slide
        // - Calculated automatically unless specified
        // - Set to 0 to disable movement along an axis
        parallaxBackgroundHorizontal: null,
        parallaxBackgroundVertical: null,

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true },
          { src: 'extensions/plugin/line-numbers/line-numbers.js' }
        ]
      });

      function handleClick(e) {
        if (1 >= outerHeight - innerHeight) {
          document.querySelector( '.reveal' ).style.cursor = 'none';
        } else {
          document.querySelector( '.reveal' ).style.cursor = '';
        }

        e.preventDefault();
        if(e.button === 0) Reveal.next();
        if(e.button === 2) Reveal.prev();
      }
    </script>

  </body>
</html>
